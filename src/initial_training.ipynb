{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from architecture.model import DLUNet\n",
    "from utils.custom_loss import Weighted_BCEnDice_loss\n",
    "from architecture.model import mean_iou, class_dice\n",
    "from utils.custom_metric import dice_coef\n",
    "from utils.load_data import BrainDataset\n",
    "from utils.training import resume_training\n",
    "\n",
    "\n",
    "# Define the directory containing the data\n",
    "train_data = r\"../data\"\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "# Initialize model, optimizer and loss\n",
    "device = torch.device(\"mps\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = DLUNet(in_channels=4).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', patience=5, factor=0.1)\n",
    "criterion = Weighted_BCEnDice_loss\n",
    "\n",
    "\n",
    "train_dataset = BrainDataset(train_data)\n",
    "val_dataset = BrainDataset(train_data, \"val\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "checkpoint_interval = 1  # Save a checkpoint every 5 epochs\n",
    "os.makedirs('model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "#     # Training phase\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     for step, (images, masks) in enumerate(train_loader, start=1):\n",
    "#         images = images.to(device)\n",
    "#         masks = masks.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(images)\n",
    "#         loss = Weighted_BCEnDice_loss(outputs, masks)\n",
    "#         dice = dice_coef(outputs, masks)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         # Log progress for training\n",
    "#         if step % 10 == 0 or step == len(train_loader):\n",
    "#             print(\n",
    "#                 f\"\\r{step}/{len(train_loader)} [==============================] - loss: {loss.item():.4f}\", end=\"\")\n",
    "#             print(\n",
    "#                 f\"\\r{step}/{len(train_loader)} [==============================] - metrics: {mean_iou(outputs, masks):.4f} c_2: {class_dice(outputs, masks, 2)} c_3: {class_dice(outputs, masks, 3)} c_4: {class_dice(outputs, masks, 4)}\", end=\"\")\n",
    "\n",
    "#     train_loss /= len(train_loader)\n",
    "#     print(f\"\\nTraining Loss: {train_loss:.4f}\")\n",
    "\n",
    "#     # Validation phase\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_dice = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for step, (images, masks) in enumerate(val_loader, start=1):\n",
    "#             images = images.to(device)\n",
    "#             masks = masks.to(device)\n",
    "\n",
    "#             outputs = model(images)\n",
    "#             loss = Weighted_BCEnDice_loss(outputs, masks)\n",
    "#             dice = dice_coef(outputs, masks)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             val_dice += dice.item()\n",
    "\n",
    "#             # Log progress for validation\n",
    "#             if step % 10 == 0 or step == len(val_loader):\n",
    "#                 print(\n",
    "#                     f\"\\rValidation {step}/{len(val_loader)} [==============================]\"\n",
    "#                     f\" - val_loss: {loss.item():.4f}\"\n",
    "#                     f\" - val_dice: {dice.item():.4f}\",\n",
    "#                     end=\"\"\n",
    "#                 )\n",
    "\n",
    "#     val_loss /= len(val_loader)\n",
    "#     val_dice /= len(val_loader)\n",
    "#     print(\n",
    "#         f\"\\nValidation Loss: {val_loss:.4f}, Validation Dice: {val_dice:.4f}\")\n",
    "\n",
    "#     # Step the scheduler with validation loss\n",
    "#     scheduler.step(val_loss)\n",
    "\n",
    "#     # Save the best model\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), 'model/dlu_net_model_best.pth')\n",
    "#         print(f\"Saved new best model with val_loss: {val_loss:.4f}\")\n",
    "\n",
    "#     # Save an intermediate checkpoint every 'checkpoint_interval' epochs\n",
    "#     if (epoch + 1) % checkpoint_interval == 0:\n",
    "#         checkpoint_path = f\"model/dlu_net_model_epoch_{epoch+1}.pth\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "#         print(\n",
    "#             f\"Intermediate checkpoint saved at epoch {epoch+1} to '{checkpoint_path}'\")\n",
    "\n",
    "#     print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/28 06:14:52 INFO mlflow.tracking.fluent: Experiment with name 'brain_segmentation' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment 'brain_segmentation' initialized with run ID: d9c9e8b4dd034ea599cb67cabd326fde\n",
      "Tracking URI: file:///Users/joe_codes/dev/school/projects/rewrite_brain_segmentation_pytourch/src/mlruns\n",
      "Loading checkpoint from ../model/slimmed_model_best.pth\n",
      "Resuming training from epoch 0\n",
      "Epoch 1/10\n",
      "689/689 [==============================] - metrics: 0.9878 c_2: 0.9383513331413269 c_3: 0.8390355706214905 c_4: 0.9452673196792603\n",
      "Training Loss: 0.1097\n",
      "Running validation on 164 batches...\n",
      "Validation 164/164 [==============================] - val_loss: 0.1348 - val_dice: 0.9875\n",
      "Validation Loss: 0.1453, Validation Dice: 0.9862\n",
      "Saved new best model with val_loss: 0.1453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/28 06:21:14 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-03-04; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'rewrite-brain-segmentation-pytourch'}\n",
      "\u001b[31m2025/04/28 06:21:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Successfully registered model 'brain_segmentation_best'.\n",
      "Created version '1' of model 'brain_segmentation_best'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model logged to MLflow with metrics: {'best_train_loss': 0.10965258961189295, 'best_train_dice': 0.9898649122787664, 'best_train_iou': 0.9820288708663298, 'best_train_class_2_dice': 0.8896188203154838, 'best_train_class_3_dice': 0.7802447084180676, 'best_train_class_4_dice': 0.9222212411322683, 'best_learning_rate': 0.0001, 'best_val_loss': 0.14534988663182025, 'best_val_dice': 0.9862056352743288, 'best_val_iou': 0.9739553782998062, 'best_val_class_2_dice': 0.8112278960463477, 'best_val_class_3_dice': 0.7513610792414445, 'best_val_class_4_dice': 0.8661600420387779, 'best_best_val_loss': 0.14534988663182025}\n",
      "Intermediate checkpoint saved at epoch 1 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_1.pth'\n",
      "------------------------------------------------------------\n",
      "Epoch 2/10\n",
      "689/689 [==============================] - metrics: 0.9842 c_2: 0.9425022602081299 c_3: 0.898888111114502 c_4: 0.94492882490158085\n",
      "Training Loss: 0.0905\n",
      "Skipping validation for epoch 2 (will validate every 10 epochs)\n",
      "Intermediate checkpoint saved at epoch 2 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_2.pth'\n",
      "------------------------------------------------------------\n",
      "Epoch 3/10\n",
      "689/689 [==============================] - metrics: 0.9887 c_2: 0.8774336576461792 c_3: 0.8360443115234375 c_4: 0.9407712817192078\n",
      "Training Loss: 0.0802\n",
      "Skipping validation for epoch 3 (will validate every 10 epochs)\n",
      "Intermediate checkpoint saved at epoch 3 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_3.pth'\n",
      "------------------------------------------------------------\n",
      "Epoch 4/10\n",
      "689/689 [==============================] - metrics: 0.9857 c_2: 0.9303509593009949 c_3: 0.919083297252655 c_4: 0.9503006339073181353\n",
      "Training Loss: 0.0731\n",
      "Skipping validation for epoch 4 (will validate every 10 epochs)\n",
      "Intermediate checkpoint saved at epoch 4 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_4.pth'\n",
      "------------------------------------------------------------\n",
      "Epoch 5/10\n",
      "689/689 [==============================] - metrics: 0.9858 c_2: 0.9420884847640991 c_3: 0.8642550110816956 c_4: 0.9496490359306335\n",
      "Training Loss: 0.0679\n",
      "Skipping validation for epoch 5 (will validate every 10 epochs)\n",
      "Intermediate checkpoint saved at epoch 5 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_5.pth'\n",
      "------------------------------------------------------------\n",
      "Epoch 6/10\n",
      "689/689 [==============================] - metrics: 0.9917 c_2: 0.9181643724441528 c_3: 0.8954357504844666 c_4: 0.9504228234291077\n",
      "Training Loss: 0.0642\n",
      "Skipping validation for epoch 6 (will validate every 10 epochs)\n",
      "Intermediate checkpoint saved at epoch 6 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_6.pth'\n",
      "------------------------------------------------------------\n",
      "Epoch 7/10\n",
      "689/689 [==============================] - metrics: 0.9875 c_2: 0.9457268118858337 c_3: 0.9030137658119202 c_4: 0.9649710655212402\n",
      "Training Loss: 0.0613\n",
      "Skipping validation for epoch 7 (will validate every 10 epochs)\n",
      "Intermediate checkpoint saved at epoch 7 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_7.pth'\n",
      "------------------------------------------------------------\n",
      "Epoch 8/10\n",
      "689/689 [==============================] - metrics: 0.9942 c_2: 0.9669081568717957 c_3: 0.9175809621810913 c_4: 0.9085190892219543\n",
      "Training Loss: 0.0590\n",
      "Skipping validation for epoch 8 (will validate every 10 epochs)\n",
      "Intermediate checkpoint saved at epoch 8 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_8.pth'\n",
      "------------------------------------------------------------\n",
      "Epoch 9/10\n",
      "689/689 [==============================] - metrics: 0.9931 c_2: 0.9194181561470032 c_3: 0.8921904563903809 c_4: 0.9612901806831364\n",
      "Training Loss: 0.0571\n",
      "Skipping validation for epoch 9 (will validate every 10 epochs)\n",
      "Intermediate checkpoint saved at epoch 9 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_9.pth'\n",
      "------------------------------------------------------------\n",
      "Epoch 10/10\n",
      "689/689 [==============================] - metrics: 0.9887 c_2: 0.9579998254776001 c_3: 0.9183360934257507 c_4: 0.9664415717124939\n",
      "Training Loss: 0.0553\n",
      "Running validation on 164 batches...\n",
      "Validation 164/164 [==============================] - val_loss: 0.1536 - val_dice: 0.9865\n",
      "Validation Loss: 0.1462, Validation Dice: 0.9866\n",
      "Validation loss did not improve. Epochs without improvement: 1\n",
      "Intermediate checkpoint saved at epoch 10 to '../model/pruned_pretrained/slimmed/dlu_net_model_epoch_10.pth'\n",
      "------------------------------------------------------------\n",
      "MLflow run ended\n"
     ]
    }
   ],
   "source": [
    "from utils.training import resume_training\n",
    "\n",
    "\n",
    "model, best_val_loss = resume_training(\n",
    "    model=model,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-4),\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_function=Weighted_BCEnDice_loss,\n",
    "    device=device,\n",
    "    resume_checkpoint_path='../model/slimmed_model_best.pth',\n",
    "    starting_epoch=0,\n",
    "    num_epochs=10,\n",
    "    checkpoint_interval=1,\n",
    "    model_save_dir='../model/pruned_pretrained/slimmed',\n",
    "    validate_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here we are pretraining models for at least 10 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell to properly load the pruned model\n",
    "from architecture.model import DLUNet, ReASPP3\n",
    "import torch\n",
    "\n",
    "# 1. Create a fresh model instance\n",
    "model = DLUNet(in_channels=4).to(device)\n",
    "\n",
    "# 2. Register ALL needed model classes with torch serialization\n",
    "torch.serialization.add_safe_globals([DLUNet, ReASPP3])\n",
    "\n",
    "# 3. Load the pruned model\n",
    "pruned_path = 'model/pruned_dlu_net.pth'\n",
    "print(f\"Loading pruned model from {pruned_path}\")\n",
    "\n",
    "try:\n",
    "    # Load with weights_only=False since we trust this file\n",
    "    checkpoint = torch.load(pruned_path, weights_only=False)\n",
    "\n",
    "    # If it's a full model (which seems to be the case)\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        # Get state_dict from the loaded model\n",
    "        model.load_state_dict(checkpoint.state_dict())\n",
    "        print(\"Successfully loaded state dict from full model object\")\n",
    "    else:\n",
    "        # If it happens to be a state_dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"Loaded state dictionary successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell for pretraining the pruned model\n",
    "# Create a fresh optimizer since we're starting a new training phase\n",
    "# Higher learning rate for fresh training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', patience=3, factor=0.1)\n",
    "\n",
    "# Don't try to load the checkpoint again in resume_training\n",
    "# Start from epoch 0 since we're pretraining\n",
    "model, best_val_loss = resume_training(\n",
    "    model=model,  # Use model we already loaded\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_function=Weighted_BCEnDice_loss,\n",
    "    device=device,\n",
    "    resume_checkpoint_path='',  # Important: leave empty to skip loading\n",
    "    starting_epoch=0,  # Start from epoch 0 for pretraining\n",
    "    num_epochs=10,  # Train for 10 epochs\n",
    "    checkpoint_interval=1,\n",
    "    # Save to a different directory\n",
    "    model_save_dir='model/pruned_pretrained/depgraph',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Training DepGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture.model import load_trained_model\n",
    "\n",
    "depgraph_model = torch.load('model/pruned_dlu_net.pth', weights_only=False)\n",
    "depgraph_model.to(device)  # Move model to GPU/MPS device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # Training phase\n",
    "    depgraph_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for step, (images, masks) in enumerate(train_loader, start=1):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = depgraph_model(images)\n",
    "        loss = Weighted_BCEnDice_loss(outputs, masks)\n",
    "        dice = dice_coef(outputs, masks)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Log progress for training\n",
    "        if step % 10 == 0 or step == len(train_loader):\n",
    "            print(\n",
    "                f\"\\r{step}/{len(train_loader)} [==============================] - loss: {loss.item():.4f}\", end=\"\")\n",
    "            print(\n",
    "                f\"\\r{step}/{len(train_loader)} [==============================] - metrics: {mean_iou(outputs, masks):.4f} c_2: {class_dice(outputs, masks, 2)} c_3: {class_dice(outputs, masks, 3)} c_4: {class_dice(outputs, masks, 4)}\", end=\"\")\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"\\nTraining Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_dice = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (images, masks) in enumerate(val_loader, start=1):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = Weighted_BCEnDice_loss(outputs, masks)\n",
    "            dice = dice_coef(outputs, masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_dice += dice.item()\n",
    "\n",
    "            # Log progress for validation\n",
    "            if step % 10 == 0 or step == len(val_loader):\n",
    "                print(\n",
    "                    f\"\\rValidation {step}/{len(val_loader)} [==============================]\"\n",
    "                    f\" - val_loss: {loss.item():.4f}\"\n",
    "                    f\" - val_dice: {dice.item():.4f}\",\n",
    "                    end=\"\"\n",
    "                )\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_dice /= len(val_loader)\n",
    "    print(\n",
    "        f\"\\nValidation Loss: {val_loss:.4f}, Validation Dice: {val_dice:.4f}\")\n",
    "\n",
    "    # Step the scheduler with validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(depgraph_model.state_dict(),\n",
    "                   'model/pruned_pretrained/depgraph_best.pth')\n",
    "        print(f\"Saved new best model with val_loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save an intermediate checkpoint every 'checkpoint_interval' epochs\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = f\"model/pruned_pretrained/depgraph{epoch+1}.pth\"\n",
    "        torch.save(depgraph_model.state_dict(), checkpoint_path)\n",
    "        print(\n",
    "            f\"Intermediate checkpoint saved at epoch {epoch+1} to '{checkpoint_path}'\")\n",
    "\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune Network slimming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the pruned model\n",
    "pruned_path = 'model/slimmed_model.pth'\n",
    "print(f\"Loading pruned model from {pruned_path}\")\n",
    "\n",
    "try:\n",
    "    # Load with weights_only=False since we trust this file\n",
    "    checkpoint = torch.load(pruned_path, weights_only=False)\n",
    "\n",
    "    # If it's a full model (which seems to be the case)\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        # Get state_dict from the loaded model\n",
    "        model.load_state_dict(checkpoint.state_dict())\n",
    "        print(\"Successfully loaded state dict from full model object\")\n",
    "    else:\n",
    "        # If it happens to be a state_dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"Loaded state dictionary successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "\n",
    "# New cell for pretraining the pruned model\n",
    "# Create a fresh optimizer since we're starting a new training phase\n",
    "# Higher learning rate for fresh training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', patience=3, factor=0.1)\n",
    "\n",
    "# Don't try to load the checkpoint again in resume_training\n",
    "# Start from epoch 0 since we're pretraining\n",
    "model, best_val_loss = resume_training(\n",
    "    model=model,  # Use model we already loaded\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_function=Weighted_BCEnDice_loss,\n",
    "    device=device,\n",
    "    resume_checkpoint_path='',  # Important: leave empty to skip loading\n",
    "    starting_epoch=0,  # Start from epoch 0 for pretraining\n",
    "    num_epochs=10,  # Train for 10 epochs\n",
    "    checkpoint_interval=1,\n",
    "    # Save to a different directory\n",
    "    model_save_dir='model/pruned_pretrained/slimmed',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNIP Prunning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the pruned model\n",
    "pruned_path = 'model/snip_pruned_model.pth'\n",
    "print(f\"Loading pruned model from {pruned_path}\")\n",
    "\n",
    "try:\n",
    "    # Load with weights_only=False since we trust this file\n",
    "    checkpoint = torch.load(pruned_path, weights_only=False)\n",
    "\n",
    "    # If it's a full model (which seems to be the case)\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        # Get state_dict from the loaded model\n",
    "        model.load_state_dict(checkpoint.state_dict())\n",
    "        print(\"Successfully loaded state dict from full model object\")\n",
    "    else:\n",
    "        # If it happens to be a state_dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"Loaded state dictionary successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "\n",
    "# New cell for pretraining the pruned model\n",
    "# Create a fresh optimizer since we're starting a new training phase\n",
    "# Higher learning rate for fresh training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', patience=3, factor=0.1)\n",
    "\n",
    "# Don't try to load the checkpoint again in resume_training\n",
    "# Start from epoch 0 since we're pretraining\n",
    "model, best_val_loss = resume_training(\n",
    "    model=model,  # Use model we already loaded\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_function=Weighted_BCEnDice_loss,\n",
    "    device=device,\n",
    "    resume_checkpoint_path='',  # Important: leave empty to skip loading\n",
    "    starting_epoch=0,  # Start from epoch 0 for pretraining\n",
    "    num_epochs=10,  # Train for 10 epochs\n",
    "    checkpoint_interval=1,\n",
    "    # Save to a different directory\n",
    "    model_save_dir='model/pruned_pretrained/snip_pruned',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8911301 8911301 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel()\n",
    "                       for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "print(total_params, trainable_params, non_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture.model import load_trained_model\n",
    "\n",
    "x = load_trained_model(\n",
    "    \"../model/slimmed_model_best.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8911301 8911301 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in x.parameters())\n",
    "trainable_params = sum(p.numel() for p in x.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "print(total_params, trainable_params, non_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
