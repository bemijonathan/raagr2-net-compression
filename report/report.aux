\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Yang2025}
\citation{Mazurek2024}
\citation{Wu2023}
\citation{Ragab2024}
\citation{Yang2025}
\citation{Ragab2024}
\citation{Rehman2023RAAGR2}
\citation{Rehman2023RAAGR2}
\citation{Wang2023Review}
\citation{Mazurek2024}
\citation{Rehman2023RAAGR2}
\citation{Yang2025}
\citation{Mazurek2024}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{6}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{6}{Introduction}{section.1}{}}
\citation{Yang2025}
\citation{Mazurek2024}
\citation{Wu2023}
\citation{Ragab2024}
\citation{Mazurek2024}
\citation{Wu2023}
\citation{Mazurek2024}
\citation{Wu2023}
\citation{Jeong2021}
\citation{Mazurek2024}
\citation{Jeong2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Theoretical Foundation and Novel Insights}{8}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Research Questions}{8}{subsection.1.2}\protected@file@percent }
\newlabel{sec:research_questions}{{1.2}{8}{Research Questions}{subsection.1.2}{}}
\citation{LeCun1990Optimal}
\citation{Han2015Learning}
\citation{Han2015deep}
\citation{Frankle2019Lottery}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Specific Objectives}{9}{subsection.1.3}\protected@file@percent }
\newlabel{sec:specific_objectives}{{1.3}{9}{Specific Objectives}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review and Related Works}{9}{section.2}\protected@file@percent }
\newlabel{sec:literature}{{2}{9}{Literature Review and Related Works}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural Network Pruning Approaches}{9}{subsection.2.1}\protected@file@percent }
\newlabel{subsec:pruning}{{2.1}{9}{Neural Network Pruning Approaches}{subsection.2.1}{}}
\citation{Cheng2024}
\citation{Cheng2024}
\citation{Frankle2019Lottery}
\citation{Gale2020Sparse}
\citation{He2017Channel}
\citation{Liu2023Survey}
\citation{Wu2023}
\citation{Gale2020Sparse}
\citation{Liu2023Survey}
\citation{Mazurek2024}
\citation{Liu2023Survey}
\citation{Mazurek2024}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison between unpruned and pruned neural networks. Left: Visualization of an unpruned network with full connectivity versus a pruned network with selective connections removed. Right: Detailed visualization of filter-level pruning mechanics showing original filter values, binary mask application, and the resulting pruned filters. The pruning process creates sparse structures while preserving essential connections. Adapted from "A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations" \cite  {Cheng2024}.}}{10}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pruning_types}{{1}{10}{Comparison between unpruned and pruned neural networks. Left: Visualization of an unpruned network with full connectivity versus a pruned network with selective connections removed. Right: Detailed visualization of filter-level pruning mechanics showing original filter values, binary mask application, and the resulting pruned filters. The pruning process creates sparse structures while preserving essential connections. Adapted from "A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations" \cite {Cheng2024}}{figure.caption.3}{}}
\citation{Guo2022Dependency}
\citation{Cai2022Dependency}
\citation{Lee2019SNIP}
\citation{Liu2023Survey}
\citation{Lee2019SNIP}
\citation{Lee2019SNIP}
\citation{Mazurek2024}
\citation{Ragab2024}
\citation{desai2024}
\citation{Krizhevsky2012ImageNet}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces SNIP (Single-shot Network Pruning) algorithm workflow. The process evaluates connection sensitivity at initialization using a single forward pass, enabling pruning before training begins. Connection sensitivity measures the impact each weight would have on the loss if removed. By keeping only the most important connections, SNIP significantly reduces computational costs compared to train-then-prune methods. Adapted from \cite  {Lee2019SNIP}.}}{12}{figure.caption.4}\protected@file@percent }
\newlabel{fig:snip_workflow}{{2}{12}{SNIP (Single-shot Network Pruning) algorithm workflow. The process evaluates connection sensitivity at initialization using a single forward pass, enabling pruning before training begins. Connection sensitivity measures the impact each weight would have on the loss if removed. By keeping only the most important connections, SNIP significantly reduces computational costs compared to train-then-prune methods. Adapted from \cite {Lee2019SNIP}}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Parameter Sharing Strategies}{12}{subsection.2.2}\protected@file@percent }
\citation{jeong2021}
\citation{li2022}
\citation{lan2020}
\citation{lan2020}
\citation{howard2017}
\citation{sandler2018}
\citation{nguyen2023}
\citation{xia2019}
\citation{kim2024}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Foundations and Evolution}{13}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Applications in Large Models}{13}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Encoder-Decoder Weight Sharing}{13}{subsubsection.2.2.3}\protected@file@percent }
\citation{jeong2021}
\citation{vu2019}
\citation{yan2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Depth-wise Parameter Sharing}{14}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Architectural comparison of traditional vs. shared depthwise convolution. Left: In traditional depthwise convolution, each input channel has its own separate filter, resulting in C×k×k parameters (where k is kernel size). Right: With shared depthwise convolution, a single set of filter weights is reused across all channels, dramatically reducing parameters to just k×k while preserving the ability to capture essential low-level features.}}{14}{figure.caption.5}\protected@file@percent }
\newlabel{fig:depthwise_sharing}{{3}{14}{Architectural comparison of traditional vs. shared depthwise convolution. Left: In traditional depthwise convolution, each input channel has its own separate filter, resulting in C×k×k parameters (where k is kernel size). Right: With shared depthwise convolution, a single set of filter weights is reused across all channels, dramatically reducing parameters to just k×k while preserving the ability to capture essential low-level features}{figure.caption.5}{}}
\citation{dabre2019}
\citation{chi2021}
\citation{howard2017}
\citation{sandler2018}
\citation{desai2024}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Implementation Considerations}{15}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Benefits and Trade-offs}{15}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Critical Analysis and Project Implications}{15}{subsubsection.2.3.3}\protected@file@percent }
\citation{desai2024}
\citation{kim2021}
\citation{Abidin2024}
\citation{Wang2023Review}
\citation{Rehman2023RAAGR2}
\citation{Rehman2023RAAGR2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Parameter Sharing vs Pruning}{16}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}U-Net Derivatives for Medical Image Segmentation}{16}{subsection.2.5}\protected@file@percent }
\citation{Oktay2018AttentionUNet}
\citation{Abidin2024}
\citation{Alom2019R2UNet}
\citation{Abidin2024}
\citation{Chen2018ASPP}
\citation{Rehman2023RAAGR2}
\citation{Rehman2023RAAGR2}
\citation{Alom2019R2UNet}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces RAAGR2-Net architecture for brain tumor segmentation. The network follows an encoder-decoder structure with progressive feature map dimensions (shown above each block). Skip connections with attention gates (AG) connect encoder features to corresponding decoder levels. The architecture combines RASP blocks for multi-scale feature extraction, recurrent residual (R2) blocks for iterative refinement, and attention mechanisms for focused segmentation. Adapted from \cite  {Rehman2023RAAGR2}.}}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:unet_evolution}{{4}{17}{RAAGR2-Net architecture for brain tumor segmentation. The network follows an encoder-decoder structure with progressive feature map dimensions (shown above each block). Skip connections with attention gates (AG) connect encoder features to corresponding decoder levels. The architecture combines RASP blocks for multi-scale feature extraction, recurrent residual (R2) blocks for iterative refinement, and attention mechanisms for focused segmentation. Adapted from \cite {Rehman2023RAAGR2}}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{17}{section.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{17}{Methodology}{section.3}{}}
\citation{Rehman2023RAAGR2}
\citation{Rehman2023RAAGR2}
\citation{Alom2019R2UNet}
\citation{Alom2019R2UNet}
\citation{Rehman2023RAAGR2}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}RAAGR2-Net Architecture}{18}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Encoder-Decoder Backbone}{18}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Recurrent CNN Blocks (RRCNNBlock)}{18}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}ReASPP3 Module}{18}{subsubsection.3.1.3}\protected@file@percent }
\citation{Chen2018ASPP}
\citation{Rehman2023RAAGR2}
\citation{Oktay2018AttentionUNet}
\citation{Oktay2018AttentionUNet}
\citation{Rehman2023RAAGR2}
\citation{Rehman2023RAAGR2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Attention Gates}{19}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Loss Function}{19}{subsubsection.3.1.5}\protected@file@percent }
\citation{Wu2023}
\citation{Han2015Learning}
\citation{Zhu2017To}
\citation{Fang2023DepGraph}
\citation{Lee2019SNIP}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Pruning Implementation}{20}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Methodological Framework for Pruning Evaluation}{20}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Magnitude-Based Pruning}{20}{subsubsection.3.2.2}\protected@file@percent }
\citation{Fang2023DepGraph}
\citation{Fang2023DepGraph}
\citation{Cai2022Dependency}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}SNIP Pruning}{21}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}DepGraph Pruning}{21}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Shared-Weight Implementation}{22}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}SharedDepthwiseBlock Design}{22}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Integration into ReASPP3}{22}{subsubsection.3.3.2}\protected@file@percent }
\citation{Jeong2021}
\citation{Vaswani2017Attention}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Parameter Reduction Analysis}{23}{subsubsection.3.3.3}\protected@file@percent }
\citation{Howard2019Searching}
\citation{Jeong2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Theoretical Relationship to Transformers}{24}{subsubsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Implementation Details}{24}{subsection.3.4}\protected@file@percent }
\newlabel{sec:implementation}{{3.4}{24}{Implementation Details}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Codebase Organization}{24}{subsubsection.3.4.1}\protected@file@percent }
\citation{Loshchilov2017}
\citation{Xin2020}
\citation{Wu2023}
\citation{Frankle2018}
\citation{Masters2018}
\citation{Sudre2017}
\citation{Micikevicius2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Training Setup}{25}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Experiment Tracking with MLFlow}{25}{subsubsection.3.4.3}\protected@file@percent }
\citation{Sudre2017}
\citation{Wang2023Review}
\citation{Abidin2024}
\citation{Chen2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Evaluation Metrics}{26}{subsubsection.3.4.4}\protected@file@percent }
\citation{Li2023}
\citation{Bakas2017}
\citation{Menze2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.5}Dataset}{27}{subsubsection.3.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{27}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{27}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Comprehensive Model Performance Tables}{28}{subsection.4.1}\protected@file@percent }
\newlabel{subsec:performance_tables}{{4.1}{28}{Comprehensive Model Performance Tables}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Performance Metrics Comparison}{28}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance Metrics Comparison Across All Model Variants}}{28}{table.caption.7}\protected@file@percent }
\newlabel{tab:performance_metrics}{{1}{28}{Performance Metrics Comparison Across All Model Variants}{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Model Statistics and Computational Benchmarks}{28}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Model Statistics and Computational Benchmarks}}{29}{table.caption.8}\protected@file@percent }
\newlabel{tab:model_benchmarks}{{2}{29}{Model Statistics and Computational Benchmarks}{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Comprehensive Reduction Analysis}{29}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Percentage Reductions Relative to Base RAAGR2-Net Model}}{29}{table.caption.9}\protected@file@percent }
\newlabel{tab:reduction_analysis}{{3}{29}{Percentage Reductions Relative to Base RAAGR2-Net Model}{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Key Insights from Tabular Analysis}{30}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Architectural Modifications Performance Analysis}{31}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Figures A-D are four panels, each with a one-sentence caption highlighting its single key message; the main text references only the essential take-aways.}}{31}{figure.caption.10}\protected@file@percent }
\newlabel{fig:comprehensive_analysis}{{5}{31}{Figures A-D are four panels, each with a one-sentence caption highlighting its single key message; the main text references only the essential take-aways}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Pruning Method Analysis}{32}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Figures A - F are are plots that show the performance of the different pruning methods.}}{32}{figure.caption.11}\protected@file@percent }
\newlabel{fig:pruning_analysis}{{6}{32}{Figures A - F are are plots that show the performance of the different pruning methods}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Resource Efficiency Analysis}{33}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Computational benchmarks comparison across model variants. Top row shows: (Left) Model Parameters in millions, (Center) Model Size in MB, (Right) Average Inference Time in ms. Bottom row shows: (Left) Computational Complexity in GFLOPs, (Center) Peak GPU Memory Usage in GB, (Right) Efficiency Improvements showing parameter reduction percentage (pink bars) and speed improvement percentage (yellow bars) relative to baseline.}}{34}{figure.caption.12}\protected@file@percent }
\newlabel{fig:resource_analysis}{{7}{34}{Computational benchmarks comparison across model variants. Top row shows: (Left) Model Parameters in millions, (Center) Model Size in MB, (Right) Average Inference Time in ms. Bottom row shows: (Left) Computational Complexity in GFLOPs, (Center) Peak GPU Memory Usage in GB, (Right) Efficiency Improvements showing parameter reduction percentage (pink bars) and speed improvement percentage (yellow bars) relative to baseline}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Training Dynamics and Convergence}{35}{subsection.4.5}\protected@file@percent }
\newlabel{fig:base_training}{{8a}{35}{Base RAAGR2-Net model (47.5 minutes)}{figure.caption.13}{}}
\newlabel{sub@fig:base_training}{{a}{35}{Base RAAGR2-Net model (47.5 minutes)}{figure.caption.13}{}}
\newlabel{fig:depthwise_training}{{8b}{35}{Depthwise Shared model (39.9 minutes, 16.0\% reduction)}{figure.caption.13}{}}
\newlabel{sub@fig:depthwise_training}{{b}{35}{Depthwise Shared model (39.9 minutes, 16.0\% reduction)}{figure.caption.13}{}}
\newlabel{fig:paired_training}{{8c}{35}{Paired Shared model (42.2 minutes, 11.2\% reduction)}{figure.caption.13}{}}
\newlabel{sub@fig:paired_training}{{c}{35}{Paired Shared model (42.2 minutes, 11.2\% reduction)}{figure.caption.13}{}}
\newlabel{fig:encdec_training}{{8d}{35}{Encoder-Decoder Shared model (46.9 minutes, 1.3\% reduction)}{figure.caption.13}{}}
\newlabel{sub@fig:encdec_training}{{d}{35}{Encoder-Decoder Shared model (46.9 minutes, 1.3\% reduction)}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Training dynamics across model variants over 35 epochs. Training times are reported for a complete 35-epoch training cycle, with percentages indicating reduction relative to the baseline model.}}{35}{figure.caption.13}\protected@file@percent }
\newlabel{fig:training_curves}{{8}{35}{Training dynamics across model variants over 35 epochs. Training times are reported for a complete 35-epoch training cycle, with percentages indicating reduction relative to the baseline model}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Comparative Convergence Analysis}{35}{subsubsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Training Efficiency and Computational Implications}{36}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Training Efficiency Comparison Across Model Variants}}{36}{table.caption.14}\protected@file@percent }
\newlabel{tab:training_efficiency}{{4}{36}{Training Efficiency Comparison Across Model Variants}{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Implications for Medical AI Development}{37}{subsubsection.4.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{37}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.0.1}Mechanisms of Enhanced Training Stability}{37}{subsubsection.5.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Theoretical Analysis of Overall vs. Class-Specific Performance Differential}{38}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Feature Hierarchy and Compression Sensitivity}{38}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Architectural Constraints and Representational Capacity}{39}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Training Dynamics and Optimization Landscape Effects}{39}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Clinical Implications of Performance Differential}{40}{subsubsection.5.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Key Findings and Implications}{40}{subsection.5.2}\protected@file@percent }
\citation{Frankle2019Lottery}
\citation{Han2015deep}
\citation{Gale2020Sparse}
\citation{Han2015deep}
\citation{Liu2023Survey}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Other Analytical Insights}{43}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{43}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{43}{Conclusion}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Key Contributions}{43}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Principal Findings}{44}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Clinical Implications}{44}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Limitations and Future Work}{45}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Critical Methodological Analysis}{45}{subsubsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Future Research Directions}{46}{subsection.6.5}\protected@file@percent }
\bibcite{Cheng2024}{1}
\bibcite{Yang2025}{2}
\bibcite{Mazurek2024}{3}
\bibcite{Wu2023}{4}
\bibcite{Ragab2024}{5}
\bibcite{Rehman2023RAAGR2}{6}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Final Remarks}{47}{subsection.6.6}\protected@file@percent }
\bibcite{Wang2023Review}{7}
\bibcite{LeCun1990Optimal}{8}
\bibcite{Han2015Learning}{9}
\bibcite{Han2015deep}{10}
\bibcite{Frankle2019Lottery}{11}
\bibcite{Gale2020Sparse}{12}
\bibcite{He2017Channel}{13}
\bibcite{Liu2023Survey}{14}
\bibcite{Cai2022Dependency}{15}
\bibcite{Fang2023DepGraph}{16}
\bibcite{Lee2019SNIP}{17}
\bibcite{desai2024}{18}
\bibcite{jeong2021}{19}
\bibcite{Krizhevsky2012ImageNet}{20}
\bibcite{li2022}{21}
\bibcite{lan2020}{22}
\bibcite{howard2017}{23}
\bibcite{sandler2018}{24}
\bibcite{nguyen2023}{25}
\bibcite{xia2019}{26}
\bibcite{kim2024}{27}
\bibcite{dabre2019}{28}
\bibcite{chi2021}{29}
\bibcite{vu2019}{30}
\bibcite{yan2024}{31}
\bibcite{kim2021}{32}
\bibcite{Abidin2024}{33}
\bibcite{Oktay2018AttentionUNet}{34}
\bibcite{Alom2019R2UNet}{35}
\bibcite{Chen2018ASPP}{36}
\bibcite{Loshchilov2017}{37}
\bibcite{Xin2020}{38}
\bibcite{Frankle2018}{39}
\bibcite{Masters2018}{40}
\bibcite{Sudre2017}{41}
\bibcite{Micikevicius2018}{42}
\bibcite{Chen2019}{43}
\bibcite{Li2023}{44}
\bibcite{Bakas2017}{45}
\bibcite{Menze2015}{46}
\bibcite{Vaswani2017Attention}{47}
\bibcite{Howard2019Searching}{48}
\bibcite{Guo2022Dependency}{49}
\bibcite{Zhu2017To}{50}
\gdef \@abspage@last{50}
