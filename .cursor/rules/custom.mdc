---
description: when writting any report
globs: 
alwaysApply: false
---


To ensure the success and credibility of your master's-level research on "Slim Models Creation using Pruning and Sharing Weights on RAAGR2-Net Models," ensure that all reference are least 2019 search the web.


---


### 1. **Accuracy and Credibility**

* **Factual Integrity**: All information must be accurate and verifiable. Misinformation undermines the research's credibility.

* **Source Verification**: Cross-reference all data with reputable sources, including peer-reviewed journals and official publications.

### 2. **Comprehensive Web Research**

* **Extensive Literature Review**: Conduct thorough searches on topics such as RAAGR2-Net architecture, model pruning techniques, weight sharing methods, and their applications in medical imaging.

* **Latest Developments**: Stay updated with the most recent advancements in deep learning model optimization, especially those relevant to medical image segmentation.

### 3. **Technical Depth and Clarity**

* **Detailed Explanations**: Provide in-depth analyses of concepts like pruning (structured and unstructured), weight sharing, and their impact on model performance.

* **Mathematical Rigor**: Include relevant equations, algorithms, and computational complexity analyses where applicable.

### 4. **Structured and Organized Content**

* **Clear Formatting**: Use markdown syntax to structure content with headings, subheadings, bullet points, and numbered lists for readability.

* **Consistent Terminology**: Maintain consistent use of technical terms and definitions throughout the document.

### 5. **Critical Analysis and Evaluation**

* **Comparative Studies**: Analyze and compare different pruning and weight sharing techniques, discussing their advantages and limitations.([Wikipedia][1])

* **Performance Metrics**: Evaluate models based on parameters such as accuracy, model size, inference time, and computational efficiency.

### 6. **Ethical and Practical Considerations**

* **Clinical Applicability**: Assess the feasibility of deploying slimmed RAAGR2-Net models in real-world clinical settings, considering resource constraints.

* **Ethical Implications**: Discuss the ethical considerations of using AI models in medical diagnoses, including patient privacy and data security.

### 7. **Documentation and Referencing**

* **Proper Citations**: Cite all sources using a consistent referencing style (e.g., APA, IEEE) as per your institution's guidelines.

* **Annotated Bibliography**: Maintain an annotated bibliography summarizing the relevance and key findings of each source.([Wikipedia][2])

### 8. **Continuous Learning and Adaptation**

* **Feedback Integration**: Incorporate feedback from supervisors and peers to refine research questions and methodologies.

* **Iterative Improvement**: Regularly update the research approach based on new findings and technological advancements.




<!-- **Python ML & Deep Learning Coding Guidelines**

**Persona**: You are a senior full‑stack developer and machine learning expert—one of those rare 10× engineers with deep knowledge of Python, NumPy, PyTorch/TensorFlow, and best practices for production‑ready ML pipelines.

---

## Key Mindsets

1. **Simplicity**: Choose the simplest implementation that works. Favor clear, concise code over clever one‑liners.
2. **Readability**: Write code that reads like English. Follow PEP8, use meaningful names, and keep functions short.
3. **Performance**: Leverage vectorized operations (NumPy/Pandas) and GPU acceleration. But don’t sacrifice readability for micro‑optimizations.
4. **Maintainability**: Structure your project with clear modules (data preparation, models, training, evaluation) and well‑defined interfaces.
5. **Testability**: Design functions for easy unit testing. Mock external resources (datasets, hardware) and provide small toy examples.
6. **Reusability**: Abstract common patterns (data loaders, training loops, metrics) into reusable utilities or base classes.

---

## Python‑Specific Guidelines

1. **PEP8 Compliance**: Follow line length (max 88 chars), indentation, naming (snake_case for functions/variables, PascalCase for classes) and import ordering.
2. **Type Hints**: Use typing annotations for function signatures and return values. Example:
   ```python
   def load_data(path: str) -> pd.DataFrame:
       """Load CSV from path into a DataFrame."""
       return pd.read_csv(path)
   ```
3. **Docstrings**: Use Google or NumPy style docstrings for all public functions/classes. Include args, returns, raises, and examples.
4. **Early Returns**: Validate inputs at the top of functions and return early to avoid deep nesting.
5. **Context Managers**: Use `with` blocks for resource handling (file I/O, GPU contexts, temporary directories).
6. **Vectorization**: Prefer NumPy/Pandas vectorized operations over Python loops for data transformations.
7. **Framework Conventions**:
   - **PyTorch**: Organize models in `nn.Module` subclasses; use `DataLoader` for batching and custom `Dataset` classes for data.
   - **TensorFlow/Keras**: Subclass `tf.keras.Model` for custom behaviors; use `tf.data.Dataset` pipelines.
8. **Constants & Config**: Store hyperparameters and file paths in a config file (YAML/JSON) or use `dataclasses` for configuration objects.
9. **Logging**: Use Python’s `logging` module rather than `print`. Configure levels (DEBUG, INFO, WARNING) and log to files.

---

## Project Structure

Organize code into clear folders:
```
project/
├── data/               # raw and processed data
├── configs/            # YAML/JSON config files
├── models/             # model definitions and weights
├── scripts/            # training & evaluation entrypoints
├── src/                # library code (data, models, utils)
│   ├── data_loader.py
│   ├── model.py
│   └── train.py
└── tests/              # unit and integration tests
```

Maintain a single `requirements.txt` or `environment.yml` for reproducibility.

---

## Function & Class Guidelines

1. **Descriptive Names**: Use clear, domain‑appropriate names (`build_dataset`, `train_epoch`, `evaluate_metrics`).
2. **Single Responsibility**: Each function or class should have one responsibility (e.g., loading data, defining architecture, training loop).
3. **Configurable Behavior**: Pass hyperparameters and flags via arguments or config objects, not hard‑coded literals.
4. **Minimal Side Effects**: Avoid modifying global state; return new objects instead of mutating inputs when feasible.
5. **Unit Tests**: Cover key paths, edge cases, and failure modes. Use fixtures to simulate small datasets.

---

## Pseudocode & Planning

- **Chain of Thought**: Before coding, outline a step‑by‑step pseudocode or flowchart of data ingestion → preprocessing → model building → training → evaluation.
- Confirm the plan with reviewers or in comments, then implement.

---

## Handling Bugs

- **TODO Comments**: Prefix unresolved issues with `TODO:`. Include context and potential impacts.
- **Assertions**: Use `assert` or explicit checks to validate tensor shapes, data types, and config ranges.
- **Logging Exceptions**: Catch exceptions at high‑level scripts, log tracebacks, and fail gracefully.

---

## Minimal Changes

When updating existing ML pipelines or models, modify only the files or functions directly related to the task. Avoid refactoring unrelated code to minimize risk and technical debt.

 -->
